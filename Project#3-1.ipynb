{"nbformat": 4, "nbformat_minor": 5, "metadata": {"language_info": {"name": "python", "version": "3.10"}, "kernelspec": {"name": "python3", "display_name": "Python 3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Information Diffusion Simulator in Clustered / Multiplex Networks\n", "\n", "This notebook implements a **Python simulator** for information diffusion\n", "through **clusters (communities)** in a (possibly) **multiplex network**.\n", "\n", "It supports:\n", "\n", "- Synthetic generation of a **two-layer multiplex network**.\n", "- Saving / loading the edge list as a **`.csv` file**.\n", "- Detection of **clusters/communities** in each layer.\n", "- Simulation of **SIR-like diffusion** with:\n", "    - Higher transmission inside clusters (`p_intra`)\n", "    - Lower transmission across clusters (`p_inter`)\n", "    - Optional **triangle-based reinforcement** (`p_tri`) inside clusters.\n", "\n", "You can plug in your own **citation network** or **Higgs multiplex network**\n", "by replacing the CSV file with real data.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# 0. Imports and basic setup\n", "\n", "import random\n", "from collections import defaultdict\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import networkx as nx\n", "import matplotlib.pyplot as plt\n", "\n", "random.seed(42)\n", "np.random.seed(42)\n", "\n", "print(\"NetworkX:\", nx.__version__)\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Generate a synthetic multiplex network and export as `.csv`\n", "\n", "We create a **two-layer multiplex network**:\n", "\n", "- Layer 0: relatively sparse\n", "- Layer 1: relatively dense\n", "\n", "The edge list is stored as a CSV with columns:\n", "\n", "- `source`\n", "- `target`\n", "- `layer`\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# 1.1 Generate multiplex network\n", "\n", "def generate_multiplex(n=200, p1=0.05, p2=0.08, seed1=1, seed2=2):\n", "    G1 = nx.fast_gnp_random_graph(n, p1, seed=seed1)\n", "    G2 = nx.fast_gnp_random_graph(n, p2, seed=seed2)\n", "    return {0: G1, 1: G2}\n", "\n", "\n", "def edges_to_df(G, layer_id):\n", "    return pd.DataFrame(\n", "        [(int(u), int(v), int(layer_id)) for u, v in G.edges()],\n", "        columns=[\"source\", \"target\", \"layer\"],\n", "    )\n", "\n", "\n", "layers = generate_multiplex()\n", "df_edges = pd.concat(\n", "    [edges_to_df(layers[0], 0), edges_to_df(layers[1], 1)],\n", "    ignore_index=True\n", ")\n", "\n", "csv_path = \"multiplex_edges.csv\"\n", "df_edges.to_csv(csv_path, index=False)\n", "print(\"Saved multiplex edge list to:\", csv_path)\n", "df_edges.head()\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Loading a multiplex network from `.csv`\n", "\n", "This function lets you:\n", "\n", "- Load the **synthetic** dataset we just saved, or\n", "- Replace `csv_path` with a path to your **Citation** or **Higgs** network edge list,\n", "  formatted as `source,target,layer`.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def load_multiplex_from_csv(path: str):\n", "    df = pd.read_csv(path)\n", "    layers = {}\n", "    for layer_id, df_layer in df.groupby(\"layer\"):\n", "        G = nx.Graph()\n", "        for _, row in df_layer.iterrows():\n", "            G.add_edge(int(row[\"source\"]), int(row[\"target\"]))\n", "        layers[int(layer_id)] = G\n", "    return layers\n", "\n", "\n", "layers_loaded = load_multiplex_from_csv(csv_path)\n", "for lid, G in layers_loaded.items():\n", "    print(f\"Layer {lid}: nodes={G.number_of_nodes()}, edges={G.number_of_edges()}, \"\n", "          f\"C={nx.average_clustering(G):.4f}\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Community / cluster detection\n", "\n", "We detect **clusters** (communities) separately in each layer using\n", "NetworkX's **greedy modularity** algorithm.\n", "\n", "Each node gets a `cluster_id` per layer.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from networkx.algorithms.community import greedy_modularity_communities\n", "\n", "def detect_clusters(G: nx.Graph):\n", "    \"\"\"Return a dict: node -> cluster_id\"\"\"\n", "    communities = list(greedy_modularity_communities(G))\n", "    cluster_map = {}\n", "    for cid, comm in enumerate(communities):\n", "        for v in comm:\n", "            cluster_map[v] = cid\n", "    return cluster_map, communities\n", "\n", "\n", "cluster_maps = {}\n", "communities = {}\n", "\n", "for lid, G in layers_loaded.items():\n", "    cmap, comms = detect_clusters(G)\n", "    cluster_maps[lid] = cmap\n", "    communities[lid] = comms\n", "    print(f\"Layer {lid}: found {len(comms)} clusters.\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Triangle enumeration (for reinforcement inside clusters)\n", "\n", "We enumerate **triangles** in each layer and build:\n", "\n", "- A list of triangles (triples of nodes)\n", "- An index `node -> triangles containing it`\n", "\n", "This will be used to apply **higher transmission** when\n", "two infected neighbors form a triangle with a susceptible node.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def enumerate_triangles(G: nx.Graph):\n", "    triangles = []\n", "    # use node ordering for canonical triple\n", "    nodes = list(G.nodes())\n", "    idx = {v: i for i, v in enumerate(nodes)}\n", "    for v in nodes:\n", "        nbrs = list(G.neighbors(v))\n", "        nbrs_sorted = sorted(nbrs, key=lambda x: idx[x])\n", "        for i in range(len(nbrs_sorted)):\n", "            for j in range(i+1, len(nbrs_sorted)):\n", "                u, w = nbrs_sorted[i], nbrs_sorted[j]\n", "                if G.has_edge(u, w):\n", "                    tri = tuple(sorted((u, v, w), key=lambda x: idx[x]))\n", "                    triangles.append(tri)\n", "    # remove duplicates\n", "    triangles = list(dict.fromkeys(triangles))\n", "    return triangles\n", "\n", "\n", "def build_triangle_index(triangles):\n", "    tri_index = defaultdict(list)\n", "    for tid, tri in enumerate(triangles):\n", "        for v in tri:\n", "            tri_index[v].append(tid)\n", "    return tri_index\n", "\n", "\n", "triangles = {}\n", "tri_index = {}\n", "\n", "for lid, G in layers_loaded.items():\n", "    tris = enumerate_triangles(G)\n", "    triangles[lid] = tris\n", "    tri_index[lid] = build_triangle_index(tris)\n", "    print(f\"Layer {lid}: triangles={len(tris)}\")\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. SIR-like diffusion through clusters\n", "\n", "We simulate a **discrete-time SIR process**:\n", "\n", "- States:  \n", "  - 0 = Susceptible (S)  \n", "  - 1 = Infected (I)  \n", "  - 2 = Recovered (R)\n", "\n", "- Parameters:\n", "  - `p_intra`: transmission probability along an edge **within the same cluster**.\n", "  - `p_inter`: transmission probability along an edge **between different clusters**.\n", "  - `p_tri`: additional probability of infection if a susceptible node is in a\n", "    triangle with **two infected neighbors in the same cluster / layer**.\n", "  - `p_recover`: per-step probability that an infected node becomes recovered.\n", "\n", "We simulate on **one chosen layer** (e.g., 0 or 1) but you can extend to\n", "multiplex coupling if needed.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def simulate_clustered_SIR(\n", "    G: nx.Graph,\n", "    clusters: dict,\n", "    triangles_layer,\n", "    tri_index_layer,\n", "    seeds,\n", "    p_intra=0.4,\n", "    p_inter=0.1,\n", "    p_tri=0.6,\n", "    p_recover=0.2,\n", "    max_steps=50,\n", "):\n", "    \"\"\"Simulate SIR with cluster-dependent and triangle-reinforced transmission.\n", "\n", "    Args:\n", "        G: underlying graph for diffusion.\n", "        clusters: dict node -> cluster_id\n", "        triangles_layer: list of triangles (u, v, w)\n", "        tri_index_layer: dict node -> list of triangle indices\n", "        seeds: iterable of initially infected nodes\n", "    Returns:\n", "        history: dict with keys 'S', 'I', 'R' (lists of counts per step)\n", "    \"\"\"\n", "    nodes = list(G.nodes())\n", "    state = {v: 0 for v in nodes}  # 0=S, 1=I, 2=R\n", "    for s in seeds:\n", "        if s in state:\n", "            state[s] = 1\n", "\n", "    S_hist, I_hist, R_hist = [], [], []\n", "\n", "    for _ in range(max_steps):\n", "        S_hist.append(sum(1 for v in nodes if state[v] == 0))\n", "        I_hist.append(sum(1 for v in nodes if state[v] == 1))\n", "        R_hist.append(sum(1 for v in nodes if state[v] == 2))\n", "\n", "        new_state = state.copy()\n", "\n", "        # Infection via edges\n", "        for u in nodes:\n", "            if state[u] != 1:\n", "                continue\n", "            for v in G.neighbors(u):\n", "                if state[v] != 0:\n", "                    continue\n", "                cu = clusters.get(u, -1)\n", "                cv = clusters.get(v, -1)\n", "                if cu == cv:\n", "                    p = p_intra\n", "                else:\n", "                    p = p_inter\n", "                if random.random() < p:\n", "                    new_state[v] = 1\n", "\n", "        # Triangle-based reinforcement\n", "        for v in nodes:\n", "            if state[v] != 0:\n", "                continue\n", "            tri_ids = tri_index_layer.get(v, [])\n", "            for tid in tri_ids:\n", "                tri = triangles_layer[tid]\n", "                others = [x for x in tri if x != v]\n", "                if len(others) != 2:\n", "                    continue\n", "                u, w = others\n", "                if state[u] == 1 and state[w] == 1:\n", "                    # Optional: require same cluster to reinforce\n", "                    if clusters.get(u, -1) == clusters.get(w, -1) == clusters.get(v, -1):\n", "                        if random.random() < p_tri:\n", "                            new_state[v] = 1\n", "                            break\n", "\n", "        # Recovery\n", "        for v in nodes:\n", "            if state[v] == 1 and random.random() < p_recover:\n", "                new_state[v] = 2\n", "\n", "        state = new_state\n", "\n", "    history = {\n", "        \"S\": S_hist,\n", "        \"I\": I_hist,\n", "        \"R\": R_hist,\n", "    }\n", "    return history\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Run example simulations on different clustering regimes\n", "\n", "We compare diffusion on:\n", "\n", "- A **randomly rewired** (low-clustering) version of the layer\n", "- The original (more clustered) version\n", "\n", "This illustrates how clustering and triangle reinforcement affect\n", "diffusion speed and final adoption.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def randomize_graph_preserve_degrees(G: nx.Graph, nswap=2000):\n", "    G_rand = G.copy()\n", "    if G_rand.number_of_edges() > 1:\n", "        nx.double_edge_swap(G_rand, nswap=nswap, max_tries=5*nswap)\n", "    return G_rand\n", "\n", "\n", "# Choose a layer for diffusion\n", "layer_for_diffusion = 0\n", "G_orig = layers_loaded[layer_for_diffusion]\n", "clusters_orig = cluster_maps[layer_for_diffusion]\n", "tris_orig = triangles[layer_for_diffusion]\n", "tri_idx_orig = tri_index[layer_for_diffusion]\n", "\n", "G_rand = randomize_graph_preserve_degrees(G_orig, nswap=2000)\n", "# Recompute clusters and triangles for randomized graph\n", "clusters_rand, comms_rand = detect_clusters(G_rand)\n", "tris_rand = enumerate_triangles(G_rand)\n", "tri_idx_rand = build_triangle_index(tris_rand)\n", "\n", "print(\"Original layer:\")\n", "print(\"  C =\", nx.average_clustering(G_orig), \"triangles =\", len(tris_orig))\n", "print(\"Randomized layer:\")\n", "print(\"  C =\", nx.average_clustering(G_rand), \"triangles =\", len(tris_rand))\n"], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {}, "source": ["# 6.1 Run diffusion on original vs randomized graph\n", "\n", "nodes = list(G_orig.nodes())\n", "seed_set = random.sample(nodes, k=5)\n", "\n", "hist_orig = simulate_clustered_SIR(\n", "    G_orig, clusters_orig, tris_orig, tri_idx_orig,\n", "    seeds=seed_set,\n", "    p_intra=0.4,\n", "    p_inter=0.1,\n", "    p_tri=0.7,\n", "    p_recover=0.2,\n", "    max_steps=40,\n", ")\n", "\n", "hist_rand = simulate_clustered_SIR(\n", "    G_rand, clusters_rand, tris_rand, tri_idx_rand,\n", "    seeds=seed_set,\n", "    p_intra=0.4,\n", "    p_inter=0.1,\n", "    p_tri=0.7,\n", "    p_recover=0.2,\n", "    max_steps=40,\n", ")\n", "\n", "t = range(len(hist_orig[\"I\"]))\n", "\n", "plt.figure()\n", "plt.plot(t, np.array(hist_orig[\"I\"]) / len(nodes), label=\"Infected (clustered)\")\n", "plt.plot(t, np.array(hist_rand[\"I\"]) / len(nodes), label=\"Infected (randomized)\", linestyle=\"--\")\n", "plt.xlabel(\"Time step\")\n", "plt.ylabel(\"Fraction infected\")\n", "plt.title(\"Information diffusion through clusters vs randomized topology\")\n", "plt.legend()\n", "plt.grid(True)\n", "plt.show()\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Parameter sweep: diffusion vs transmissibility\n", "\n", "Finally, we sweep over different intra-cluster transmission probabilities\n", "`p_intra` and measure final adoption (fraction recovered) to see how\n", "clustering changes the effective **diffusion threshold**.\n"]}, {"cell_type": "code", "metadata": {}, "source": ["def run_threshold_sweep(\n", "    G, clusters, tris, tri_idx,\n", "    p_intra_values,\n", "    p_inter=0.1,\n", "    p_tri=0.7,\n", "    p_recover=0.2,\n", "    num_seeds=5,\n", "    num_reps=5,\n", "    max_steps=40,\n", "):\n", "    nodes = list(G.nodes())\n", "    n = len(nodes)\n", "    final_R = []\n", "    for pin in p_intra_values:\n", "        vals = []\n", "        for _ in range(num_reps):\n", "            seeds = random.sample(nodes, k=num_seeds)\n", "            hist = simulate_clustered_SIR(\n", "                G, clusters, tris, tri_idx,\n", "                seeds=seeds,\n", "                p_intra=pin,\n", "                p_inter=p_inter,\n", "                p_tri=p_tri,\n", "                p_recover=p_recover,\n", "                max_steps=max_steps,\n", "            )\n", "            vals.append(hist[\"R\"][-1] / n)\n", "        final_R.append(np.mean(vals))\n", "    return np.array(final_R)\n", "\n", "\n", "p_values = np.linspace(0.0, 1.0, 21)\n", "\n", "final_R_orig = run_threshold_sweep(\n", "    G_orig, clusters_orig, tris_orig, tri_idx_orig,\n", "    p_intra_values=p_values,\n", ")\n", "\n", "final_R_rand = run_threshold_sweep(\n", "    G_rand, clusters_rand, tris_rand, tri_idx_rand,\n", "    p_intra_values=p_values,\n", ")\n", "\n", "plt.figure()\n", "plt.plot(p_values, final_R_orig, label=\"Clustered\")\n", "plt.plot(p_values, final_R_rand, label=\"Randomized\", linestyle=\"--\")\n", "plt.xlabel(\"Intra-cluster transmission $p_{intra}$\")\n", "plt.ylabel(\"Final adoption (R/N)\")\n", "plt.title(\"Effect of clustering on diffusion threshold\")\n", "plt.legend()\n", "plt.grid(True)\n", "plt.show()\n"], "outputs": [], "execution_count": null}]}